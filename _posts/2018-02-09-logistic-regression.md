---
layout: post
title: 逻辑回归解决分类问题
date: 2018-02-09 14:33:56
tags: [逻辑回归, 分类问题]
categories: 机器学习
mathjax: true
---

* content
{:toc}

本文对使用逻辑回归解决分类问题的原理进行介绍。




在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误）。分类问题的例子有：判断一封电子邮件是否是垃圾邮件；判断一次金融交易是否是欺诈；以及肿瘤分类问题，区别一个肿瘤是恶性的还是良性的。

## 二元的分类问题
### 逻辑回归模型
我们将因变量（dependant variable）可能属于的两个类分别称为负向类（negative class）和
正向类(positive class)，则因变量 $y\in \{0,1\}$，其中 $0$ 表示负向类，$1$ 表示正向类。

因此，逻辑回归模型的输出变量范围应始终在 $0$ 和 $1$ 之间。逻辑回归模型的假设是：$$h\_θ(x)=g(θ^TX)$$

* $X$ 代表特征向量
* $g$ 代表逻辑函数（logistic function），是一个常用的 $S$ 形函数（Sigmoid function)，公式为：$$g(z)=\frac{1}{1+e^{-z}}$$

该函数图像为：
![S形函数图像](http://ouy59qaqh.bkt.clouddn.com/sigmoid_function.jpg)

### 代价函数
首先，我们知道线性回归的代价函数是：$$J(\theta)=\frac{1}{m}\sum\_{i=1}^{m}\frac{1}{2}(h\_\theta(x^{(i)})-y^{(i)})^2$$我们重新定义逻辑回归的代价函数为：$$J(\theta)=\frac{1}{m}\sum\_{i=1}^{m}Cost(h\_\theta(x^{(i)}),y^{(i)})$$其中，
$$
Cost(h\_\theta(x),y)=\begin {cases}
-log(h\_\theta(x)), & y = 1 \\\\
-log(1 - h\_\theta(x)), & y = 0
\end {cases}
$$
这样构建的 $Cost(h\_θ(x),y)$ 函数的特点是：

* 当实际的 $y=1$ 且 $h\_θ$ 也为 $1$ 时，误差为 $0$，当 $y=1$ 但 $h\_θ$ 不为 $1$ 时，误差随着 $h\_θ$ 的变小而变大；
* 当实际的 $y=0$ 且 $h\_θ$ 也为 $0$ 时，代价为 $0$，当 $y=0$ 但 $h\_θ$ 不为 $0$ 时，误差随着 $h\_θ$ 的变大而变大。

上面的函数可以合并为：$$Cost(h\_\theta(x),y)=-y\cdot log(h\_\theta(x))-(1-y)\cdot log(1-h\_\theta(x))$$即，逻辑回归的代价函数：$$J(\theta)=\frac{1}{m}\sum\_{i=1}^{m}Cost(h\_\theta(x^{(i)}),y^{(i)})=-\frac{1}{m}[\sum\_{i=1}^{m}y^{(i)}log h\_\theta(x^{(i)})+(1-y^{(i)})log(1-h\_\theta(x^{(i)}))]$$根据这个代价函数，为了拟合出参数，我们要试图找尽量让 $J(\theta)$ 取得最小值的参数 $\theta$。

最小化代价函数的方法，是使用梯度下降法（gradient descent）。推导之后我们可以得到：$$\theta\_j:=\theta\_j-\alpha\frac{1}{m}\sum\_{i=1}^{m}((h\_\theta(x^{(i)})-y^{(i)})\cdot x\_{j}^{(i)})$$通过这个式子来更新所有的 $\theta$ 值。

在这里，我们可能会发现这个式子正是我们用来做线性回归梯度下降的。那么线性回归和逻辑回归是否是统一算法。要回答这个问题，我们需要看看逻辑回归发生了哪些变化。实际上，假设的定义发生了变化。

* 对于线性回归假设函数：$h\_\theta(x)=\theta^TX$
* 而现在逻辑回归假设函数：$h\_\theta(x)=\frac{1}{1+e^{-\theta^T}}$

因此，即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑 函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。

## 多类别分类：一对多
![一对多示例](http://ouy59qaqh.bkt.clouddn.com/one_vs_all.jpg)

现在我们有一个训练集，好比上图表示的有三个类别，我们用三角形表示 $y=1$，方框表示 $y=2$，叉叉表示 $y=3$。我们下面要做的就是使用一个训练集，将其分成三个二元分类问题。我们先从用三角形代表的类别 $1$ 开始，实际上我们可以创建一个新的“伪”训练集，类型 $2$ 和类型 $3$ 定为负类，类型 $1$ 设定为正类，我们创建一个新的训练集，如下图所示的那
样，我们要拟合出一个合适的分类器。

![一对多示例--1](http://ouy59qaqh.bkt.clouddn.com/one_vs_all_1.jpg)

这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为 $1$，圆形的值为 $0$，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。为了能实现这样的转变，我们将多个类中的一个类标记为正向类($y=1$)，然后将其他 所有类都标记为负向类，这个模型记作 $h\_\theta^{(1)}(x)$。接着，类似地第我们选择另一个类标记为正向类($y=2$)，再将其它类都标记为负向类，将这个模型记作 $h\_\theta^{(2)}(x)$,依此类推。最后我们得到一系列的模型简记为：$$h\_\theta^{(i)}(x)=p(y=i|x;\theta)\qquad i=(1,2,3...k)$$![一对多示例--2](http://ouy59qaqh.bkt.clouddn.com/one_vs_all_2.jpg)

最后，在我们需要做预测时，我们将所有的分类机都运行一遍，选择 $max(h\_\theta^{(i)}(x))$，则得到对应的分类。
